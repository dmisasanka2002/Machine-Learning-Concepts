{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naive Bayes Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Naive Bayes algorithm** is a probabilistic classification algorithm based on **Bayes' Theorem**. It is called \"Naive\" because it assumes that the features are independent of each other given the class label, which is rarely true in real-world scenarios.\n",
    "\n",
    "- **Key Idea**: Use probabilities to predict the class of a given input.\n",
    "- **Applications**: Spam detection, sentiment analysis, document classification, and medical diagnosis.\n",
    "\n",
    "<br><br>\n",
    "![Application of naive bayes.png](../images/appli_of_naive_bayes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Key Mathematical Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Bayes' Theorem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "- \\(P(A|B)\\): Probability of event \\(A\\) (class) given event \\(B\\) (data).\n",
    "- \\(P(B|A)\\): Probability of event \\(B\\) given event \\(A\\).\n",
    "- \\(P(A)\\): Prior probability of event \\(A\\) (class).\n",
    "- \\(P(B)\\): Prior probability of event \\(B\\) (data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Naive Bayes Assumption**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a dataset with features \\(X = (x_1, x_2, $\\dots$, x_n)\\), the algorithm assumes that features are conditionally independent:\n",
    "\n",
    "$$\n",
    "P(X|C) = P(x_1|C) \\cdot P(x_2|C) \\cdot \\dots \\cdot P(x_n|C)\n",
    "$$\n",
    "\n",
    "Thus, the **posterior probability** is:\n",
    "\n",
    "$$\n",
    "P(C|X) = \\frac{P(C) \\cdot \\prod_{i=1}^{n} P(x_i|C)}{P(X)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(C\\): Class label.\n",
    "- \\(P(C)\\): Prior probability of the class.\n",
    "- \\(P(x_i|C)\\): Conditional probability of feature \\(x_i\\) given class \\(C\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Types of Naive Bayes Classifiers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.1 Gaussian Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for **`continuous data`**, assuming features follow a normal (Gaussian) distribution:\n",
    "\n",
    "$$\n",
    "P(x_i|C) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\n",
    "$$\n",
    "\n",
    "- \\($\\mu$\\): Mean of the feature.\n",
    "- \\($\\sigma^2$\\): Variance of the feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Multinomial Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for **`discrete data`**, especially for text classification and document categorization (e.g., bag-of-words).\n",
    "\n",
    "$$\n",
    "P(x_i|C) = \\frac{\\text{Count}(x_i, C) + \\alpha}{\\text{Total Words in } C + \\alpha \\cdot |V|}\n",
    "$$\n",
    "\n",
    "- \\($\\alpha$\\): Smoothing parameter (Laplace smoothing).\n",
    "- \\(|V|\\): Vocabulary size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3 Bernoulli Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for **`binary data`**. It models the presence or absence of features:\n",
    "\n",
    "$$\n",
    "P(x_i|C) = P(x_i = 1|C)^x_i \\cdot P(x_i = 0|C)^{1-x_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Gaussian Naive Bayes Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Iris Classification\n",
    "\n",
    "```python\n",
    "# Import Libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load Dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into Training and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Gaussian Naive Bayes Classifier\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train the Model\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = gnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Gaussian Naive Bayes Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Multinomial Naive Bayes Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Text Classification\n",
    "\n",
    "```python\n",
    "# Import Libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Sample Text Data\n",
    "texts = [\"I love programming\", \"Python is great\", \"I hate bugs\", \"Debugging is fun\", \"I enjoy learning\"]\n",
    "labels = [1, 1, 0, 0, 1]  # 1: Positive, 0: Negative\n",
    "\n",
    "# Convert Text to Feature Vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Split into Training and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Multinomial Naive Bayes Classifier\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Train the Model\n",
    "mnb.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = mnb.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Multinomial Naive Bayes Accuracy: {accuracy:.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.3 Bernoulli Naive Bayes Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Binary Features\n",
    "\n",
    "```python\n",
    "# Import Libraries\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "import numpy as np\n",
    "\n",
    "# Binary Feature Dataset\n",
    "X = np.random.randint(2, size=(10, 5))\n",
    "y = np.random.randint(2, size=10)\n",
    "\n",
    "# Initialize Bernoulli Naive Bayes Classifier\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Train the Model\n",
    "bnb.fit(X, y)\n",
    "\n",
    "# Predict\n",
    "y_pred = bnb.predict(X)\n",
    "\n",
    "print(\"Predictions:\", y_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Advantages and Disadvantages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Advantages**\n",
    "\n",
    "- **Fast and Efficient**: Works well with large datasets.\n",
    "\n",
    "- **Simple Implementation**: Easy to understand and use.\n",
    "\n",
    "- **Effective for Text Data**: Performs well in document **classification tasks**.\n",
    "\n",
    "- **Probabilistic Output**: Provides class probabilities for better interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Disadvantages**\n",
    "\n",
    "- **Feature Independence Assumption**: Real-world data often violates this assumption.\n",
    "\n",
    "- **Sensitive to Zero Probabilities**: Requires smoothing techniques to handle zero probabilities.\n",
    "\n",
    "- **Not Suitable for Complex Relationships**: Performs poorly when features are heavily correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Extensions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Laplace Smoothing\n",
    "\n",
    "To avoid zero probabilities in Multinomial Naive Bayes:\n",
    "\n",
    "$$\n",
    "P(x_i|C) = \\frac{\\text{Count}(x_i, C) + \\alpha}{\\text{Total Count in Class } C + \\alpha \\cdot |V|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Hybrid Models\n",
    "\n",
    "Combine Naive Bayes with other algorithms (e.g., Naive Bayes + SVM) for better performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Naive Bayes is a simple yet effective algorithm for probabilistic classification.\n",
    "\n",
    "- It works well for high-dimensional data, especially text-based datasets.\n",
    "\n",
    "- While it has limitations due to its independence assumption, it remains a popular choice for fast and scalable applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
