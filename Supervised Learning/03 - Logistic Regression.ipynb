{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is a statistical analysis method used to predict a data value based on prior observations of a data set. It is used for Classification problems.\n",
    "\n",
    "Logistic regression is a method for predicting binary outcomes (0 or 1). It estimates the probability of the occurrence of an event using a logistic function, which outputs values between 0 and 1. It is a **classification algorithm**, as opposed to regression, which predicts continuous values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why \"Logistic\" Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The term \"logistic\" comes from the logistic function, also known as the sigmoid function, which maps any input value into a range between 0 and 1.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Function (Sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic function, also called the sigmoid function, is a mathematical function that maps ***real-valued input to a value between 0 and 1***. It is defined as:\n",
    "\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( $\\sigma(z)$ \\) is the sigmoid function.\n",
    "- \\( z \\) is the linear combination of input features: \\( z = w_0 + w_1x_1 + w_2x_2 + $\\cdots$ + w_nx_n \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sigmoid Function.png](../images/sigmoid_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of Sigmoid:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Output Range**: The output is always between 0 and 1, making it suitable for probability estimation.\n",
    "- **S-shaped curve**: The function has a characteristic \"S\" shape, where it sharply transitions from 0 to 1 as the input moves from negative to positive values.\n",
    "\n",
    "The sigmoid function can be interpreted as the probability of a certain class (e.g., class 1) given the input features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression model can be represented as:\n",
    "\n",
    "$$\n",
    "P(y = 1|X) = \\sigma(w_0 + w_1x_1 + w_2x_2 + \\cdots + w_nx_n)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( P(y=1|X) \\) is the probability that the output \\( y \\) is 1, given the input features \\( X \\).\n",
    "- \\( $\\sigma$ \\) is the sigmoid function.\n",
    "- \\( w_0 \\) is the bias term (intercept).\n",
    "- \\( w_1, w_2, $\\ldots$, w_n \\) are the weights for the corresponding features.\n",
    "\n",
    "The output of logistic regression is a probability, and typically, a threshold of 0.5 is used to decide the predicted class:\n",
    "- If \\( P(y = 1|X) $\\geq$ 0.5 \\), predict class 1.\n",
    "- If \\( P(y = 1|X) < 0.5 \\), predict class 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of logistic regression is to find the parameters \\( w_0, w_1, $\\dots$, w_n \\) that minimize the cost function. For logistic regression, we use the **log-loss** (or **binary cross-entropy**) cost function, defined as:\n",
    "\n",
    "$$\n",
    "J(w) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( m \\) is the number of training examples.\n",
    "- \\( $y^{(i)}$ \\) is the true label for the \\( i \\)-th example.\n",
    "- \\( $h_{\\theta}(x^{(i)})$ = $\\sigma(w_0 + w_1x_1 + \\cdots + w_nx_n)$ \\) is the predicted probability for the \\( i \\)-th example.\n",
    "- \\( $\\log$ \\) is the natural logarithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function Intuition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the model predicts a value close to the true label, the cost will be small.\n",
    "- If the model predicts a value far from the true label, the cost will be large.\n",
    "- The goal is to minimize this cost using optimization techniques like **Gradient Descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a logistic regression model, we need to adjust the model parameters (weights) to minimize the cost function. The most common approach is **Gradient Descent**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Gradient Descent** is an iterative optimization algorithm used to minimize the cost function by updating the weights in the opposite direction of the gradient.\n",
    "\n",
    "The update rule is:\n",
    "\n",
    "$$\n",
    "w_j = w_j - \\alpha \\cdot \\frac{\\partial J(w)}{\\partial w_j}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( $\\alpha$ \\) is the learning rate.\n",
    "- \\( $\\frac{\\partial J(w)}{\\partial w_j} $\\) is the partial derivative of the cost function with respect to the parameter \\( $w_j$ \\).\n",
    "\n",
    "The weights are updated in each iteration until convergence, i.e., when the cost function stops decreasing significantly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization is a technique used to prevent overfitting, especially when the model has many parameters or when there is multicollinearity among the features. Logistic regression uses two types of regularization:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. **L1 Regularization (Lasso)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L1 regularization term adds the absolute value of the coefficients to the cost function:\n",
    "\n",
    "$$\n",
    "J(w) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right] + \\lambda \\sum_{j=1}^{n} |w_j|\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( $\\lambda$ \\) is the regularization parameter that controls the strength of regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. **L2 Regularization (Ridge)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularization adds the squared value of the coefficients to the cost function:\n",
    "\n",
    "$$\n",
    "J(w) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left[ y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right] + \\lambda \\sum_{j=1}^{n} w_j^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\( $\\lambda$ \\) is the regularization parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Problem Statement\n",
    "Prevent overfitting on a dataset with many features using L1 (Lasso) and L2 (Ridge) regularization.\n",
    "\n",
    "- #### Implementation\n",
    "```python\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# L2 Regularization (default)\n",
    "model_l2 = LogisticRegression(penalty=\"l2\", solver=\"lbfgs\", max_iter=200)\n",
    "model_l2.fit(X_train, y_train)\n",
    "print(\"L2 Regularization Accuracy:\", model_l2.score(X_test, y_test))\n",
    "\n",
    "# L1 Regularization\n",
    "model_l1 = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", max_iter=200)\n",
    "model_l1.fit(X_train, y_train)\n",
    "print(\"L1 Regularization Accuracy:\", model_l1.score(X_test, y_test))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts in Regularization\n",
    "- **L1 Regularization**: Encourages sparsity by adding the absolute value of coefficients to the loss function.\n",
    "- **L2 Regularization**: Penalizes large coefficients by adding their squared values to the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When evaluating a logistic regression model, [this](../Concepts/07%20-%20models_evaluation_methods.ipynb) metrics are typically used:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.2f}\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Confusion Matrix**: Shows TP, FP, TN, FN counts.\n",
    "- **ROC Curve**: Plots TPR against FPR at various thresholds.\n",
    "- **AUC (Area Under Curve)**: Summarizes the ROC curve into a single value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Problem Statement\n",
    "We aim to predict whether a student will pass or fail based on their study hours.\n",
    "\n",
    "- #### Dataset\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Sample dataset\n",
    "data = {\n",
    "    \"Hours_Studied\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    \"Passed\": [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Features and target\n",
    "X = df[[\"Hours_Studied\"]]\n",
    "y = df[\"Passed\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts in Binary Logistic Regression\n",
    "- Use of a sigmoid function to model probabilities.\n",
    "- Threshold-based decision-making (e.g., \\( P(y=1|X) \\geq 0.5 \\)).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Implement Binary Logistic Regression Model](04%20-%20Implement%20Logistic%20Regression%20Model.ipynb)\n",
    "\n",
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression can be extended to handle multiple classes using one of two common strategies:\n",
    "- **One-vs-All (OvA)**: Train one binary classifier per class, where each classifier distinguishes between a specific class and the rest.\n",
    "- **Softmax Regression**: This generalizes logistic regression to multiclass problems, where the probability of each class is modeled using a softmax function.\n",
    "\n",
    "The softmax function is defined as:\n",
    "\n",
    "$$\n",
    "P(y = k | X) = \\frac{e^{z_k}}{\\sum_{j=1}^{K} e^{z_j}}\n",
    "$$\n",
    "\n",
    "Where \\( z_k \\) is the score for class \\( k \\), and \\( K \\) is the total number of classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Problem Statement\n",
    "Predict the species of flowers based on their features using the Iris dataset.\n",
    "\n",
    "- #### Implementation\n",
    "```python\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Multiclass Logistic Regression model\n",
    "model = LogisticRegression(multi_class=\"multinomial\", solver=\"lbfgs\", max_iter=200)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts in Multiclass Logistic Regression\n",
    "- **One-vs-All (OvA)**: Train one binary classifier per class.\n",
    "- **Softmax Regression**: Generalization of logistic regression for multiple classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Implement Multiclass Logistic Regression Model](04%20-%20Implement%20Logistic%20Regression%20Model.ipynb)\n",
    "\n",
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Medical Diagnosis**: Predicting whether a patient has a certain disease based on medical data.\n",
    "- **Email Spam Detection**: Classifying emails as spam or not spam.\n",
    "- **Credit Scoring**: Predicting whether a customer will default on a loan.\n",
    "- **Customer Churn Prediction**: Predicting whether a customer will cancel a subscription."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages:\n",
    "- Simple to implement and understand.\n",
    "- Outputs probabilities, making it interpretable.\n",
    "- Works well for binary classification problems.\n",
    "- Can be regularized to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages:\n",
    "- Assumes a linear relationship between input features and log-odds.\n",
    "- Can be underpowered for more complex relationships.\n",
    "- Sensitive to outliers in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Logistic regression is a powerful and interpretable tool for binary classification. By understanding the logistic function, cost function, and evaluation metrics, one can effectively apply logistic regression to real-world classification tasks. Regularization and extension to multiclass problems make logistic regression versatile for many applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
