{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Random Forest Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Random Forest** is an ensemble learning algorithm that combines multiple decision trees to improve prediction accuracy and prevent overfitting. It is widely used for both **classification** and **regression** tasks.\n",
    "<br><br>\n",
    "\n",
    "![Random Forest.png](../images/random_forest.png)\n",
    "\n",
    "- **Key Idea**: Aggregate predictions from multiple trees to make a final prediction.\n",
    "- **Why Random?**\n",
    "  1. **Random Subset of Features**: At each split, only a random subset of features is considered.\n",
    "  2. **Random Sampling**: Data is sampled with replacement (bootstrapping) to train individual trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Key Mathematical Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Ensemble Learning**\n",
    "Random Forest uses **Bagging** (Bootstrap Aggregating) to combine predictions from multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging Formula:\n",
    "$$\n",
    "\\hat{y} = \\frac{1}{T} \\sum_{t=1}^{T} f_t(x)\n",
    "$$\n",
    "- \\(T\\): Number of trees.\n",
    "- $(f_t(x))$: Prediction from tree \\(t\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Feature Randomness**\n",
    "For a dataset with \\(F\\) features, only \\($\\sqrt{F}$\\) (for classification) or \\(F/3\\) (for regression) features are used for splitting at each node. This introduces diversity among trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Comparison: Decision Tree vs Random Forest**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "| **Aspect**              | **Decision Tree**                                      | **Random Forest**                                   |\n",
    "|--------------------------|-------------------------------------------------------|---------------------------------------------------|\n",
    "| **Overfitting**          | Prone to overfitting.                                 | Reduces overfitting by averaging multiple trees.  |\n",
    "| **Accuracy**             | May have lower accuracy due to high variance.         | Generally higher accuracy.                        |\n",
    "| **Interpretability**     | Easy to interpret and visualize.                      | Harder to interpret as it aggregates multiple trees. |\n",
    "| **Training Speed**       | Faster to train.                                      | Slower due to training multiple trees.            |\n",
    "| **Feature Importance**   | Provides direct feature importance.                  | Provides averaged feature importance.             |\n",
    "| **Robustness**           | Sensitive to noisy data.                              | Robust to noise and outliers.                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Classification Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Iris Classification\n",
    "\n",
    "This example predicts the type of iris flower using a Random Forest Classifier.\n",
    "\n",
    "```python\n",
    "# Import libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load Iris Dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split into Training and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Random Forest Classifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Model\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Feature Importance\n",
    "feature_importance = pd.Series(clf.feature_importances_, index=iris.feature_names)\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance.sort_values(ascending=False))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Implement of Random Forest Algorithm](10%20-%20Implement%20Random%20Forest%20Algorithm.ipynb)\n",
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Regression Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: California Housing Prices\n",
    "\n",
    "This example predicts house prices using a Random Forest Regressor.\n",
    "\n",
    "```python\n",
    "#Import libraries\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "\n",
    "# Load California Housing Dataset\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split into Training and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Initialize Random Forest Regressor\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the Model\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Implement of Random Forest Algorithm](10%20-%20Implement%20Random%20Forest%20Algorithm.ipynb)\n",
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Visualization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Feature Importance Visualization\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize Feature Importance\n",
    "feature_importance.sort_values().plot(kind='barh', title=\"Feature Importance\", figsize=(10, 6))\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Mathematical Formulations**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.1 Random Sampling**\n",
    "For 'N' samples in the dataset, Random Forest creates multiple training datasets by sampling 'N' samples with replacement (bootstrapping)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.2 Aggregation of Predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification:\n",
    "\n",
    "$$\\hat{y}​=Mode(y1​,y2​,…,yT​)$$\n",
    "\n",
    "  - Takes the majority vote of *T* trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression:\n",
    "\n",
    "$$\\hat{y} = \\frac{1}{T} \\sum_{t=1}^{T} y_t$$\n",
    "\n",
    "  - Takes the average of predictions from *T* trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.3 Out-of-Bag (OOB) Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses samples not included in the bootstrap to evaluate model performance:\n",
    "\n",
    "$$OOB \\, Error = \\frac{1}{N} \\sum_{i=1}^{N} L(y_i, \\hat{y}_i^{OOB})$$\n",
    "\n",
    "  - L: Loss function (e.g., accuracy or MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Advantages and Disadvantages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages**\n",
    "\n",
    "- Reduces overfitting compared to Decision Trees.\n",
    "\n",
    "- Handles large datasets with higher accuracy.\n",
    "\n",
    "- Robust to noise and missing data.\n",
    "\n",
    "- Provides feature importance rankings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disadvantages**\n",
    "\n",
    "- Requires more computational resources than a single Decision Tree.\n",
    "\n",
    "- Harder to interpret due to the ensemble nature.\n",
    "\n",
    "- May not perform well with sparse data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Advantages & Disadvantages.png](../images/random_forest_advantages.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **8. Comparison Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Decision Tree Classifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(max_depth=5, random_state=42)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "\n",
    "print(f\"Decision Tree Accuracy: {dt_accuracy:.2f}\")\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy:.2f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **9. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Takeaways\n",
    "\n",
    "- Random Forest is a more robust and accurate model compared to a single Decision Tree due to ensemble learning.\n",
    "\n",
    "- It minimizes overfitting and improves generalization by combining predictions from multiple trees.\n",
    "\n",
    "- Decision Trees are faster and easier to interpret but less accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. Extensions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GridSearchCV or RandomizedSearchCV to optimize hyperparameters like:\n",
    "\n",
    "- Number of estimators (n_estimators).\n",
    "\n",
    "- Maximum depth of trees (max_depth).\n",
    "\n",
    "- Minimum samples split (min_samples_split)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble Variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Boosting: Sequentially builds trees to minimize residual errors.\n",
    "\n",
    "- XGBoost: An optimized version of Gradient Boosting.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'n_estimators': [50, 100, 200], 'max_depth': [None, 10, 20]}\n",
    "grid_search = GridSearchCV(RandomForestClassifier(random_state=42), params, cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
