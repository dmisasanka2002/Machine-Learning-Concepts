{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Decision Tree Algorithm** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Introduction** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Decision Tree** is a supervised learning algorithm used for **classification** and **regression** tasks. It recursively splits the dataset into smaller subsets based on feature values, creating a tree-like structure. \n",
    "- **Classification Decision Trees**: Predict categorical outcomes (e.g., spam or not spam). \n",
    "- **Regression Decision Trees**: Predict continuous values (e.g., house prices). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Key Mathematical Concepts** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Splitting Criteria** \n",
    "Decision trees split nodes using specific mathematical criteria to determine the \"best\" split. Common criteria include: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. **Gini Impurity (Classification)** \n",
    "Measures the likelihood of an incorrect classification at a node: \n",
    "$$ \n",
    "    Gini = 1 - \\sum_{i=1}^{C} p_i^2 \n",
    "$$ \n",
    "- \\(p_i\\): Proportion of instances belonging to class \\(i\\). \n",
    "- \\(C\\): Total number of classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Entropy (Classification)** \n",
    "Entropy measures uncertainty or impurity in a node: \n",
    "$$ \n",
    "    Entropy = - \\sum_{i=1}^{C} p_i \\cdot \\log_2(p_i) \n",
    "$$ \n",
    "- Lower entropy indicates purer splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Information Gain (Classification)** \n",
    "The reduction in entropy due to splitting: \n",
    "$$ \n",
    "    Information \\, Gain = Entropy(parent) - \\sum_{k=1}^{K} \\frac{|D_k|}{|D|} \\cdot Entropy(D_k) \n",
    "$$ \n",
    "- \\( D_k \\): Subset of data after the split. \n",
    "- \\( |D_k| \\): Number of samples in \\(D_k\\). \n",
    "- \\( |D| \\): Total samples in the parent node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Mean Squared Error (MSE) (Regression)** \n",
    "Measures the variance within a node for regression tasks: \n",
    "$$ \n",
    "    MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y})^2 \n",
    "$$ \n",
    "\n",
    "Where:\n",
    "- \\( y_i \\): True value. \n",
    "- \\( $\\hat{y}$ \\): Predicted value. \n",
    "- \\( n \\): Number of samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Tree Pruning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pruning reduces overfitting by limiting the size of the tree.\n",
    "  - **Pre-pruning**: Stop growing the tree early (e.g., limit depth, minimum samples per split).\n",
    "  - **Post-pruning**: Remove branches after the tree is fully grown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **max_depth**: Maximum depth of the tree.\n",
    "2. **min_samples_split**: Minimum samples required to split a node.\n",
    "3. **criterion**: The function used to measure the quality of a split (`gini` or `entropy`).\n",
    "4. **max_features**: Number of features to consider when splitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Decision Tree Structure** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Root Node**: The starting point of the tree where data splits. \n",
    "2. **Internal Nodes**: Intermediate decision points. \n",
    "3. **Leaf Nodes**: Terminal nodes providing the output. \n",
    "\n",
    "Example: \n",
    "For a dataset predicting whether to play tennis:\n",
    "- Root Node: Weather\n",
    "- Internal Nodes: Outlook, Humidity\n",
    "- Leaf Nodes: Play (Yes/No)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree Example.png](../images/dta_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But there are so many ways to create the decision trees for one problem. \n",
    "<br>\n",
    "<img src = \"../images/dta_ways.png\" width = 70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the correct way or Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Read More](#21-splitting-criteria)\n",
    "\n",
    "Ex : By using **Entropy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Decision Tree Entropy Calculation](../images/dta_entropy_calc.png)\n",
    "<br><br>\n",
    "- Lower entropy tree is used to develop the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Implementation** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: Iris Classification\n",
    "\n",
    "This example predicts the type of iris flower using a Decision Tree Classifier.\n",
    "\n",
    "```python\n",
    "# Import Libraries\n",
    "from sklearn.datasets import load_iris, fetch_california_housing \n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree, export_text \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, mean_squared_error \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Iris Dataset\n",
    "iris = load_iris() \n",
    "X, y = iris.data, iris.target \n",
    "\n",
    "# Split into Training and Test Sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) \n",
    "\n",
    "# Initialize Decision Tree Classifier \n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=42) \n",
    "\n",
    "# Train the Model \n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate \n",
    "y_pred = clf.predict(X_test) \n",
    "accuracy = accuracy_score(y_test, y_pred) \n",
    "print(f\"Accuracy: {accuracy:.2f}\") \n",
    "\n",
    "# Visualize the Decision Tree \n",
    "plt.figure(figsize=(12, 8)) \n",
    "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True) \n",
    "plt.show() \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Implementation Of Decision Tree Algorithm](08%20-%20Implement%20Decision%20Tree%20Algorithm.ipynb#decision-tree-algorithm-for-classification-problem)\n",
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset: California Housing Prices\n",
    "\n",
    "This example predicts house prices using a Decision Tree Regressor.\n",
    "\n",
    "```python\n",
    "# Load California Housing Dataset \n",
    "data = fetch_california_housing() \n",
    "X, y = data.data, data.target \n",
    "\n",
    "# Split into Training and Test Sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) \n",
    "\n",
    "# Initialize Decision Tree Regressor \n",
    "regressor = DecisionTreeRegressor(max_depth=5, random_state=42) \n",
    "\n",
    "# Train the Model \n",
    "regressor.fit(X_train, y_train) \n",
    "\n",
    "# Predict and Evaluate \n",
    "y_pred = regressor.predict(X_test) \n",
    "mse = mean_squared_error(y_test, y_pred) \n",
    "print(f\"Mean Squared Error: {mse:.2f}\") \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Implementation Of Decision Tree Algorithm](08%20-%20Implement%20Decision%20Tree%20Algorithm.ipynb#decision-tree-algorithm-for-regression-problem)\n",
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Visualization and Export**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Plotting the Tree\n",
    "\n",
    "```python\n",
    "from sklearn.tree import plot_tree \n",
    "\n",
    "# Visualize the Decision Tree \n",
    "plt.figure(figsize=(12, 8)) \n",
    "plot_tree(clf, feature_names=iris.feature_names, class_names=iris.target_names, filled=True) \n",
    "plt.show() \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Exporting the Tree as Text\n",
    "\n",
    "```python\n",
    "from sklearn.tree import export_text \n",
    "\n",
    "# Export the Tree as Text \n",
    "tree_rules = export_text(clf, feature_names=iris.feature_names) \n",
    "print(tree_rules) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Common Issues and Solutions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Overfitting\n",
    "\n",
    "- Trees can grow excessively, leading to overfitting.\n",
    "\n",
    "- Solution: Prune the tree or use hyperparameter constraints: \n",
    "  - Use pruning\n",
    "\n",
    "  - Limit max_depth.\n",
    "\n",
    "  - Set min_samples_split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Imbalanced Data\n",
    "\n",
    "- Trees may be biased toward majority classes.\n",
    "\n",
    "- Solution: \n",
    "\n",
    "  - Use class weights (class_weight='balanced').\n",
    "\n",
    "  - Resample the data (e.g., SMOTE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **7. Mathematical Formulation of Splits**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Gini Impurity Calculation:\n",
    "\n",
    "Consider a node with:\n",
    "\n",
    "- Class 1: 4 samples\n",
    "\n",
    "- Class 2: 6 samples\n",
    "\n",
    "$$    \n",
    "    Gini = 1 - \\left(\\frac{4}{10}\\right)^2 - \\left(\\frac{6}{10}\\right)^2 = 1 - 0.16 - 0.36 = 0.48 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### Mean Squared Error Calculation:\n",
    "\n",
    "For a node with predictions:\n",
    "\n",
    "- Actual: y=[3,5,4]\n",
    "\n",
    "- Predicted Mean: $\\hat{y} = 4$\n",
    "\n",
    "$$\n",
    "    MSE = \\frac{1}{3} \\left((3 - 4)^2 + (5 - 4)^2 + (4 - 4)^2\\right) = \\frac{1}{3} (1 + 1 + 0) = 0.67 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Advantages and Disadvantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Advantages\n",
    "\n",
    "- Easy to interpret and visualize.\n",
    "\n",
    "- Handles both numerical and categorical data.\n",
    "\n",
    "- Requires minimal data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ### Disadvantages\n",
    "\n",
    "- Prone to overfitting (requires pruning or setting constraints).\n",
    "\n",
    "- Sensitive to small variations in data (can lead to different splits).\n",
    "\n",
    "- Not optimal for very large datasets.\n",
    "\n",
    "- Requires careful tuning of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random Forest:\n",
    "\n",
    "  - Combines multiple decision trees to improve accuracy.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42) \n",
    "rf.fit(X_train, y_train) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Gradient Boosting:\n",
    "\n",
    "  - Builds trees sequentially to correct errors of previous trees.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier \n",
    "gb = GradientBoostingClassifier(random_state=42) \n",
    "gb.fit(X_train, y_train) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. XGBoost:\n",
    "\n",
    "  - Optimized version of Gradient Boosting.\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier \n",
    "xgb = XGBClassifier(random_state=42) \n",
    "xgb.fit(X_train, y_train) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Parameter         | Description                             |\n",
    "|------------------|-----------------------------------------|\n",
    "|max_depth         |Maximum depth of the tree.               |\n",
    "|min_samples_split |Minimum samples required to split a node.|\n",
    "|criterion         |Splitting criteria (`gini, entropy`).    |\n",
    "|splitter          |Splitter method (`best, random`).        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are a powerful, interpretable model for supervised learning tasks. Their effectiveness can be improved through pruning and ensemble techniques like Random Forests or Boosting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "- Decision trees use splitting criteria like Gini Impurity, Entropy, and MSE.\n",
    "\n",
    "- They are prone to overfitting without proper tuning.\n",
    "\n",
    "- They are interpretable and versatile, suitable for both classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
