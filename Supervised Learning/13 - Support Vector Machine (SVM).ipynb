{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Support Vector Machine (SVM) Algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Support Vector Machine (SVM)** is a powerful supervised learning algorithm used for both classification and regression tasks. It is particularly effective for high-dimensional datasets and when the classes are well-separated.\n",
    "\n",
    "- **Key Idea**: Find the optimal **hyperplane** that separates data into different classes with the maximum margin.\n",
    "- **Applications**: Image classification, text categorization, bioinformatics, and handwriting recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Application of SVM.png](../images/svm_applications.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **2. Key Mathematical Concepts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.1 Hyperplane**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **hyperplane** is a decision boundary that separates different classes. For \\(n\\)-dimensional data, the hyperplane is \\(n-1\\)-dimensional.\n",
    "\n",
    "For two-dimensional data:\n",
    "$$\n",
    "w \\cdot x + b = 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(w\\): Weight vector (normal to the hyperplane).\n",
    "- \\(x\\): Feature vector.\n",
    "- \\(b\\): Bias term.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![HyperPlanes.png](../images/hyperplanes.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2 Maximum Margin**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM aims to **maximize the margin**, which is the distance between the hyperplane and the nearest data points (support vectors). \n",
    "\n",
    "The margin is defined as:\n",
    "$$\n",
    "\\text{Margin} = \\frac{2}{\\|w\\|}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SVM.png](../images/hyperplanes_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3 Optimization Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To maximize the margin, SVM minimizes:\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} \\|w\\|^2\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1, \\quad \\forall i\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\($y_i$\\): Class label (\\(+1\\) or \\(-1\\)).\n",
    "- \\($x_i$\\): Feature vector of the \\($i^{th}$\\) sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.4 Kernel Trick**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `non-linearly separable data`, SVM uses the **kernel trick** to project data into a `higher-dimensional` space where a linear hyperplane can separate the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Common Kernel Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Linear Kernel**:\n",
    "   $$\n",
    "   K(x_i, x_j) = x_i \\cdot x_j\n",
    "   $$\n",
    "\n",
    "2. **Polynomial Kernel**:\n",
    "   $$\n",
    "   K(x_i, x_j) = (\\gamma x_i \\cdot x_j + r)^d\n",
    "   $$\n",
    "\n",
    "3. **Radial Basis Function (RBF) Kernel**:\n",
    "   $$\n",
    "   K(x_i, x_j) = \\exp(-\\gamma \\|x_i - x_j\\|^2)\n",
    "   $$\n",
    "\n",
    "4. **Sigmoid Kernel**:\n",
    "   $$\n",
    "   K(x_i, x_j) = \\tanh(\\gamma x_i \\cdot x_j + r)\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **3. Types of SVM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Hard Margin SVM**: Assumes data is linearly separable without errors.\n",
    "2. **Soft Margin SVM**: Allows some misclassification by introducing a penalty term.\n",
    "\n",
    "Objective function for Soft Margin SVM:\n",
    "\n",
    "$$\n",
    "\\min \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{n} \\xi_i\n",
    "$$\n",
    "\n",
    "Subject to:\n",
    "\n",
    "$$\n",
    "y_i (w \\cdot x_i + b) \\geq 1 - \\xi_i, \\quad \\xi_i \\geq 0\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- \\(C\\): Regularization parameter.\n",
    "- \\($\\xi_i$\\): Slack variables for misclassification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **4. Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.1 Linear SVM Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**: Synthetic Data\n",
    "\n",
    "```python\n",
    "# Import Libraries\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Generate Synthetic Dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_classes=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train Linear SVM\n",
    "linear_svm = SVC(kernel='linear', C=1.0)\n",
    "linear_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = linear_svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Linear SVM Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Decision Boundary**\n",
    "```python\n",
    "# Import Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the Decision Boundary\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
    "w = linear_svm.coef_[0]\n",
    "b = linear_svm.intercept_[0]\n",
    "\n",
    "# Calculate decision boundary\n",
    "x_points = np.linspace(X[:, 0].min(), X[:, 0].max(), 100)\n",
    "decision_boundary = -(w[0] / w[1]) * x_points - b / w[1]\n",
    "\n",
    "plt.plot(x_points, decision_boundary, color='black')\n",
    "plt.title(\"Linear SVM Decision Boundary\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Non-Linear SVM Example**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset**: Synthetic Data (Non-Linear)\n",
    "\n",
    "```python\n",
    "# Import Libraries\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Generate Non-Linear Dataset\n",
    "X, y = make_moons(n_samples=200, noise=0.1, random_state=42)\n",
    "\n",
    "# Split Dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train RBF Kernel SVM\n",
    "rbf_svm = SVC(kernel='rbf', C=1.0, gamma=0.5)\n",
    "rbf_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = rbf_svm.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RBF SVM Accuracy: {accuracy:.2f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Decision Boundary**\n",
    "```python\n",
    "# Import Libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the Decision Boundary for RBF Kernel\n",
    "xx, yy = np.meshgrid(np.linspace(X[:, 0].min(), X[:, 0].max(), 100),\n",
    "                     np.linspace(X[:, 1].min(), X[:, 1].max(), 100))\n",
    "Z = rbf_svm.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "\n",
    "plt.contourf(xx, yy, Z, levels=np.linspace(Z.min(), Z.max(), 50), cmap='coolwarm', alpha=0.8)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap='coolwarm', edgecolors='k')\n",
    "plt.title(\"RBF Kernel SVM Decision Boundary\")\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **5. Advantages and Disadvantages**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "- **Effective in High Dimensions**: Handles datasets with many features effectively.\n",
    "\n",
    "- **Robust to Overfitting**: Especially with large margins.\n",
    "\n",
    "- **Versatile**: Supports linear and non-linear classification via kernel trick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages\n",
    "- **Computationally Expensive**: Training is slow for large datasets.\n",
    "\n",
    "- **Difficult to Interpret**: Non-linear SVMs are hard to interpret compared to decision trees.\n",
    "\n",
    "- **Requires Careful Tuning**: Parameters like CCC and γ\\gammaγ significantly impact performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **6. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM is a powerful algorithm for both linear and non-linear classification tasks.\n",
    "\n",
    "- It is effective for high-dimensional data but computationally expensive.\n",
    "\n",
    "- Careful parameter tuning (e.g., CCC, kernel type) is essential for optimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
