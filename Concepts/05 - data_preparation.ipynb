{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Preparation in Machine Learning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation is a fundamental step in any machine learning pipeline. It ensures the quality and reliability of the data being fed into the models. This process involves data collection, cleaning, transformation, and splitting.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. **Data Collection**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources of Data:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Structured Data**: Databases, CSV files, etc.\n",
    "- **Unstructured Data**: Text, images, videos, etc.\n",
    "- **APIs**: Open datasets via REST APIs (e.g., Kaggle, Open Data Portal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tools for Data Collection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Python libraries like `requests`, `BeautifulSoup` (for web scraping).\n",
    "- Tools like Postman for API testing.\n",
    "- Databases such as MySQL, MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data.csv')  # Load data from a CSV file\n",
    "print(data.head())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. **Data Cleaning**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleaning involves removing or correcting inaccuracies in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps in Data Cleaning:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Handling **missing values**:\n",
    "  - Replace with mean/median/mode.\n",
    "  - Drop rows/columns.\n",
    "- Removing **duplicates**.\n",
    "- Handling **outliers**: [Outliers Detection Methods](5.2%20-%20data_cleaning_process.ipynb)\n",
    "  - Use Z-score or IQR methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# Handling missing values\n",
    "data.fillna(data.mean(), inplace=True)  # Replace NaN with mean\n",
    "\n",
    "# Removing duplicates\n",
    "data.drop_duplicates(inplace=True)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Pandas Examples 1](../../Python%20Libraries/pandas/pandas_ex1.ipynb)\n",
    "- [Pandas Examples 2](../../Python%20Libraries/pandas/pandas_ex2.ipynb)\n",
    "\n",
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Data Transformation**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transformation prepares data for better performance in machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Normalization and Standardization**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalization:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Scales the data to a fixed range, typically [0, 1].\n",
    "- Useful when features have different units or scales. <br><br>\n",
    "\n",
    "![Normalization.png](../images/normalization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data_normalized = scaler.fit_transform(data)\n",
    "print(data_normalized[:5])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardization:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Centers the data around zero with a standard deviation of 1.\n",
    "- Useful when the data follows a Gaussian distribution. <br><br>\n",
    "\n",
    "![Standardization.png](../images/standardization.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data_standardized = scaler.fit_transform(data)\n",
    "print(data_standardized[:5])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Encoding Categorical Variables**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Hot Encoding:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converts categorical variables into a binary matrix.\n",
    "- Avoids ordinal relationships between categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![One-hot encording.png](../images/one-hot%20encording_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'Color': ['Red', 'Blue', 'Green']})\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "data_encoded = encoder.fit_transform(data).toarray()\n",
    "print(data_encoded)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python Implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ##### **Using OneHotEncoder Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'Result': ['Pass', 'Fail', 'Pass', \"Pass\", \"Absent\", \"Fail\", \"Fail\", \"Pass\", \"Pass\", \"Absent\", \"Pass\"]})\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "data_encoded = encoder.fit_transform(data).toarray()\n",
    "print(data_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **encoder.categories_** : Return the order of encode dataset when use **OneHotEncoder** Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['Absent', 'Fail', 'Pass'], dtype=object)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.categories_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ##### **Using LabelBinarizer Class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [0 1 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = pd.DataFrame({'Result': ['Pass', 'Fail', 'Pass', \"Pass\", \"Absent\", \"Fail\", \"Fail\", \"Pass\", \"Pass\", \"Absent\", \"Pass\"]})\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "data_encoded = encoder.fit_transform(data)\n",
    "print(data_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **encoder.classes_** : Return the order of encode dataset when use **LabelBinarizer** Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Absent', 'Fail', 'Pass'], dtype='<U6')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[See more about *Data Transformation Tecniques*](./5.1%20-%20advance_data_transformation.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Encoding:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Assigns an integer value to each category.\n",
    "- Suitable for ordinal variables.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Lable Encording.png](../images/label-encording_example.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "data['Color_encoded'] = encoder.fit_transform(data['Color'])\n",
    "print(data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Python Implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Result  Result_encoded\n",
      "0     Pass               2\n",
      "1     Fail               1\n",
      "2     Pass               2\n",
      "3     Pass               2\n",
      "4   Absent               0\n",
      "5     Fail               1\n",
      "6     Fail               1\n",
      "7     Pass               2\n",
      "8     Pass               2\n",
      "9   Absent               0\n",
      "10    Pass               2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "data['Result_encoded'] = encoder.fit_transform(data['Result'])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Data Splitting**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is split into training, validation, and test sets to evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose of Splits:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Training Set**: Used to train the model.\n",
    "- **Validation Set**: Used to tune hyperparameters and prevent overfitting.\n",
    "- **Test Set**: Used to evaluate the final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Splits:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Training: 70%-80% of the data.\n",
    "- Validation: 10%-15%.\n",
    "- Testing: 10%-15%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Code:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Example dataset\n",
    "X = data.drop('target', axis=1)  # Features\n",
    "y = data['target']  # Target variable\n",
    "\n",
    "# Splitting into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further splitting training set into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Actual Python Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20])\n",
    "y_vals = np.array([10, 12, 8, 7.5, 4.6, 2.5, 15.8, 19.7, 1.2, 3.8, 6.3, 14.1, 11.3, 13.7, 16.1, 17, 18.6, 20, 5, 9.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13 15 11  7  6  5 18  1 19  8  4 12  2  3  9]\n",
      "[16 14 17 20 10]\n",
      "[11.3 16.1  6.3 15.8  2.5  4.6 20.  10.   5.  19.7  7.5 14.1 12.   8.\n",
      "  1.2]\n",
      "[17.  13.7 18.6  9.1  3.8]\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_vals, y_vals)\n",
    "print(x_train) # defalt 75% of x values\n",
    "print(x_test) # defalt 25% of x values\n",
    "print(y_train) # defalt 75% of y values\n",
    "print(y_test) # defalt 25% of y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15 18 10 13 14  1  4  2  3  8 19  6 17 12  9 16]\n",
      "[11 20  7  5]\n",
      "[16.1 20.   3.8 11.3 13.7 10.   7.5 12.   8.  19.7  5.   2.5 18.6 14.1\n",
      "  1.2 17. ]\n",
      "[ 6.3  9.1 15.8  4.6]\n"
     ]
    }
   ],
   "source": [
    "# defining training size of the dataset. \n",
    "# The 'test_size' parameter of train_test_split must be a float in the range (0.0, 1.0)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_vals, y_vals, test_size=0.2) \n",
    "print(x_train) # 80% of x values\n",
    "print(x_test) # 20% of x values\n",
    "print(y_train) # 80% of y values\n",
    "print(y_test) # 20% of y values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Step               | Description                                        | Tools/Libraries    |\n",
    "|--------------------|----------------------------------------------------|--------------------|\n",
    "| Data Collection    | Gather data from various sources.                 | Pandas, APIs       |\n",
    "| Data Cleaning      | Remove inaccuracies, handle missing values.       | Pandas, NumPy      |\n",
    "| Data Transformation| Normalize, standardize, encode categorical data.  | Scikit-learn       |\n",
    "| Data Splitting     | Split data into training, validation, test sets.  | Scikit-learn       |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Scikit-learn Documentation](\n",
    "https://scikit-learn.org/stable/documentation.html\n",
    ")\n",
    "- [Pandas User Guide](\n",
    "https://pandas.pydata.org/docs/\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Data Preparation Example](5.1%20-%20advance_data_transformation.ipynb#data-preprocessing-with-example-datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
